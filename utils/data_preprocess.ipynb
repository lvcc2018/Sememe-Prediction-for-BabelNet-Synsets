{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import wordnet,stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import thulac\n",
    "import pickle\n",
    "import re\n",
    "import jieba\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# File paths\n",
    "babel_data_file = '../data/babel_data_full.txt'    # BaBelNet原始数据\n",
    "babel_sememe_file = '../data/synset_sememes.txt'   # BabelSememe原始数据\n",
    "data = '../data/babel_data'                        # BabelNet原始数据词典\n",
    "clean_data = '../data/data_clean'                  # BabelNet清洗后的数据"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "# Utils\n",
    "wnl = WordNetLemmatizer()\n",
    "englist_stop_words = set(stopwords.words('english'))\n",
    "pattern = {'zh':re.compile(r'[^\\u4e00-\\u9fa5]'),'en':re.compile('[^a-z^A-Z^\\s]')}\n",
    "chinese_stop_words = [i[:-1] for i in open('../data/Chinese_stop_words').readlines()]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Read source file, get babel_data\n",
    "def read_list(line):\n",
    "    line = line[:-1].split('\\t')\n",
    "    num = int(line[0])\n",
    "    assert(num == len(line[1:]))\n",
    "    return line[1:]\n",
    "\n",
    "def read_synset(f):\n",
    "    synset = {}\n",
    "    synset['id'] = f.readline()\n",
    "    if not synset['id']:\n",
    "        return\n",
    "    synset['id'] = synset['id'][:-1]\n",
    "    for k in ['w_e','w_c','w_f']:\n",
    "        synset[k] = read_list(f.readline())\n",
    "    for k in ['d_e_m','d_c_m','d_f_m']:\n",
    "        synset[k] = f.readline()[:-1]\n",
    "    for k in ['d_e','d_c','d_f']:\n",
    "        synset[k] = read_list(f.readline())\n",
    "    synset['i_m'] = f.readline()[:-1]\n",
    "    synset['i'] = read_list(f.readline())\n",
    "    return synset\n",
    "\n",
    "def read_babel_data(f):\n",
    "    babel_data ={}\n",
    "    while True:\n",
    "        d = read_synset(f)\n",
    "        if not d:\n",
    "            return babel_data\n",
    "        babel_data[d['id']] = d\n",
    "    \n",
    "def read_babel_sememe(f):\n",
    "    lines = f.readlines()\n",
    "    babel_sememe = {line[:-1].split()[0] : line[:-1].split()[1:] for line in lines}\n",
    "    return babel_sememe\n",
    "\n",
    "def get_babel_data():\n",
    "    babel_data = read_babel_data(open(babel_data_file))\n",
    "    babel_sememe = read_babel_sememe(open(babel_sememe_file))\n",
    "\n",
    "    for k in tqdm(babel_data):\n",
    "        babel_data[k]['s'] = babel_sememe[k]\n",
    "    return babel_data\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "babel_data = get_babel_data()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 15755/15755 [00:00<00:00, 1266506.81it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "list(babel_data.values())[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'id': 'bn:00113968a',\n",
       " 'w_e': ['yellow', 'yellowish', 'xanthous'],\n",
       " 'w_c': ['黄', '黄色', '淡黄色+的', '黄色+的', '微黄色', '微黄色+的', '黄+的', '淡黄色'],\n",
       " 'w_f': ['jaune'],\n",
       " 'd_e_m': 'Of the color intermediate between green and orange in the color spectrum; of something resembling the color of an egg yolk',\n",
       " 'd_c_m': '像丝瓜花或向日葵花的颜色。',\n",
       " 'd_f_m': \"Qui a la couleur du jaune d'œuf, d'un citron ou de l'or.\",\n",
       " 'd_e': ['Of the color intermediate between green and orange in the color spectrum; of something resembling the color of an egg yolk',\n",
       "  'Of the color intermediate between green and orange in the color spectrum; of something resembling the color of an egg yolk',\n",
       "  'Having the colour of a yolk, a lemon or gold.'],\n",
       " 'd_c': ['像丝瓜花或向日葵花的颜色。'],\n",
       " 'd_f': [\"Qui a la couleur du jaune d'œuf, d'un citron ou de l'or.\"],\n",
       " 'i_m': '',\n",
       " 'i': [],\n",
       " 's': ['yellow|黄']}"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "def clean_word_list(word_list, lang='en'):\n",
    "    res = []\n",
    "    for w in word_list:\n",
    "        if lang == 'en':\n",
    "            w = wnl.lemmatize(w.lower())\n",
    "            w = re.sub(pattern['en'], '', w)\n",
    "            if w not in englist_stop_words:\n",
    "                res.append(w)\n",
    "        elif lang == 'zh':\n",
    "            w = re.sub(pattern['zh'],'',w)\n",
    "            if w not in chinese_stop_words:\n",
    "                res.append(w)\n",
    "    return res\n",
    "\n",
    "def split_sentence(sentence, lang='zh'):\n",
    "    if lang=='zh':\n",
    "        return jieba.lcut(sentence)\n",
    "    else:\n",
    "        return sentence.split(' ')\n",
    "            \n",
    "def get_clean_data(babel_data):\n",
    "    clean_data ={}\n",
    "    for k in tqdm(babel_data.keys()):\n",
    "        clean_data_instance = {}\n",
    "        clean_data_instance['w_e'] = clean_word_list(babel_data[k]['w_e'], lang='en')\n",
    "        clean_data_instance['w_c'] = clean_word_list(babel_data[k]['w_c'], lang='zh')\n",
    "        clean_data_instance['w_f'] = babel_data[k]['w_f']\n",
    "        clean_data_instance['d_e_m'] = clean_word_list(split_sentence(babel_data[k]['d_e_m'],lang='en'), lang='en')\n",
    "        clean_data_instance['d_c_m'] = clean_word_list(split_sentence(babel_data[k]['d_c_m'],lang='zh'), lang='zh')\n",
    "        clean_data_instance['d_f_m'] = split_sentence(babel_data[k]['d_f_m'],lang='fr')\n",
    "        clean_data_instance['d_e'] = [clean_word_list(split_sentence(i,lang='en'), lang='en') for i in babel_data[k]['d_e']]\n",
    "        clean_data_instance['d_c'] = [clean_word_list(split_sentence(i,lang='zh'), lang='zh') for i in babel_data[k]['d_c']]\n",
    "        clean_data_instance['d_f'] = [split_sentence(i,lang='fr') for i in babel_data[k]['d_f']]\n",
    "        for i in ['i_m','i','s']:\n",
    "            clean_data_instance[i] = babel_data[k][i]\n",
    "        clean_data[k] = clean_data_instance\n",
    "    return clean_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "clean_data = get_clean_data(babel_data)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Dump cache file failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/jieba/__init__.py\", line 154, in initialize\n",
      "    _replace_file(fpath, cache_file)\n",
      "PermissionError: [Errno 1] Operation not permitted: '/tmp/tmpxyk8owky' -> '/tmp/jieba.cache'\n",
      "Loading model cost 0.776 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "list(clean_data.values())[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'w_e': ['yellow', 'yellowish', 'xanthous'],\n",
       " 'w_c': ['黄', '黄色', '淡黄色的', '黄色的', '微黄色', '微黄色的', '黄的', '淡黄色'],\n",
       " 'w_f': ['jaune'],\n",
       " 'd_e_m': ['color',\n",
       "  'intermediate',\n",
       "  'green',\n",
       "  'orange',\n",
       "  'color',\n",
       "  'spectrum',\n",
       "  'something',\n",
       "  'resembling',\n",
       "  'color',\n",
       "  'egg',\n",
       "  'yolk'],\n",
       " 'd_c_m': ['丝瓜', '花', '向日葵', '花', '颜色'],\n",
       " 'd_f_m': ['Qui',\n",
       "  'a',\n",
       "  'la',\n",
       "  'couleur',\n",
       "  'du',\n",
       "  'jaune',\n",
       "  \"d'œuf,\",\n",
       "  \"d'un\",\n",
       "  'citron',\n",
       "  'ou',\n",
       "  'de',\n",
       "  \"l'or.\"],\n",
       " 'd_e': [['color',\n",
       "   'intermediate',\n",
       "   'green',\n",
       "   'orange',\n",
       "   'color',\n",
       "   'spectrum',\n",
       "   'something',\n",
       "   'resembling',\n",
       "   'color',\n",
       "   'egg',\n",
       "   'yolk'],\n",
       "  ['color',\n",
       "   'intermediate',\n",
       "   'green',\n",
       "   'orange',\n",
       "   'color',\n",
       "   'spectrum',\n",
       "   'something',\n",
       "   'resembling',\n",
       "   'color',\n",
       "   'egg',\n",
       "   'yolk'],\n",
       "  ['colour', 'yolk', 'lemon', 'gold']],\n",
       " 'd_c': [['丝瓜', '花', '向日葵', '花', '颜色']],\n",
       " 'd_f': [['Qui',\n",
       "   'a',\n",
       "   'la',\n",
       "   'couleur',\n",
       "   'du',\n",
       "   'jaune',\n",
       "   \"d'œuf,\",\n",
       "   \"d'un\",\n",
       "   'citron',\n",
       "   'ou',\n",
       "   'de',\n",
       "   \"l'or.\"]],\n",
       " 'i_m': '',\n",
       " 'i': [],\n",
       " 's': ['yellow|黄']}"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "pickle.dump(clean_data, open('../data/clean_data','wb'))"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}