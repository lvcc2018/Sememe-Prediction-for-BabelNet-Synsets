{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import wordnet,stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import thulac\n",
    "import pickle\n",
    "import re\n",
    "import jieba\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# File paths\n",
    "babel_data_file = '../data/babel_data_full.txt'    # BaBelNet原始数据\n",
    "babel_sememe_file = '../data/synset_sememes.txt'   # BabelSememe原始数据\n",
    "data = '../data/babel_data'                        # BabelNet原始数据词典\n",
    "clean_data = '../data/data_clean'                  # BabelNet清洗后的数据"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Utils\n",
    "wnl = WordNetLemmatizer()\n",
    "englist_stop_words = set(stopwords.words('english'))\n",
    "pattern = {'zh':re.compile(r'[^\\u4e00-\\u9fa5]'),'en':re.compile('[^a-z^A-Z^\\s]')}\n",
    "chinese_stop_words = [i[:-1] for i in open('../data/Chinese_stop_words').readlines()]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Read source file, get babel_data\n",
    "def read_list(line):\n",
    "    line = line[:-1].split('\\t')\n",
    "    num = int(line[0])\n",
    "    assert(num == len(line[1:]))\n",
    "    return line[1:]\n",
    "\n",
    "def read_synset(f):\n",
    "    synset = {}\n",
    "    synset['id'] = f.readline()\n",
    "    if not synset['id']:\n",
    "        return\n",
    "    synset['id'] = synset['id'][:-1]\n",
    "    for k in ['w_e','w_c','w_f']:\n",
    "        synset[k] = read_list(f.readline())\n",
    "    for k in ['d_e_m','d_c_m','d_f_m']:\n",
    "        synset[k] = f.readline()[:-1]\n",
    "    for k in ['d_e','d_c','d_f']:\n",
    "        synset[k] = read_list(f.readline())\n",
    "    synset['i_m'] = f.readline()[:-1]\n",
    "    synset['i'] = read_list(f.readline())\n",
    "    return synset\n",
    "\n",
    "def read_babel_data(f):\n",
    "    babel_data ={}\n",
    "    while True:\n",
    "        d = read_synset(f)\n",
    "        if not d:\n",
    "            return babel_data\n",
    "        babel_data[d['id']] = d\n",
    "    \n",
    "def read_babel_sememe(f):\n",
    "    lines = f.readlines()\n",
    "    babel_sememe = {line[:-1].split()[0] : line[:-1].split()[1:] for line in lines}\n",
    "    return babel_sememe\n",
    "\n",
    "def get_babel_data():\n",
    "    babel_data = read_babel_data(open(babel_data_file))\n",
    "    babel_sememe = read_babel_sememe(open(babel_sememe_file))\n",
    "\n",
    "    for k in tqdm(babel_data):\n",
    "        babel_data[k]['s'] = babel_sememe[k]\n",
    "    return babel_data\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "babel_data = get_babel_data()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "list(babel_data.values())[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def clean_word_list(word_list, lang='en'):\n",
    "    res = []\n",
    "    for w in word_list:\n",
    "        if lang == 'en':\n",
    "            w = wnl.lemmatize(w.lower())\n",
    "            w = re.sub(pattern['en'], '', w)\n",
    "            if w not in englist_stop_words:\n",
    "                res.append(w)\n",
    "        elif lang == 'zh':\n",
    "            w = re.sub(pattern['zh'],'',w)\n",
    "            if w not in chinese_stop_words:\n",
    "                res.append(w)\n",
    "    return res\n",
    "\n",
    "def split_sentence(sentence, lang='zh'):\n",
    "    if lang=='zh':\n",
    "        return jieba.lcut(sentence)\n",
    "    else:\n",
    "        return sentence.split(' ')\n",
    "            \n",
    "def get_clean_data(babel_data):\n",
    "    clean_data ={}\n",
    "    for k in tqdm(babel_data.keys()):\n",
    "        clean_data_instance = {}\n",
    "        clean_data_instance['w_e'] = clean_word_list(babel_data[k]['w_e'], lang='en')\n",
    "        clean_data_instance['w_c'] = clean_word_list(babel_data[k]['w_c'], lang='zh')\n",
    "        clean_data_instance['w_f'] = babel_data[k]['w_f']\n",
    "        clean_data_instance['d_e_m'] = clean_word_list(split_sentence(babel_data[k]['d_e_m'],lang='en'), lang='en')\n",
    "        clean_data_instance['d_c_m'] = clean_word_list(split_sentence(babel_data[k]['d_c_m'],lang='zh'), lang='zh')\n",
    "        clean_data_instance['d_f_m'] = split_sentence(babel_data[k]['d_f_m'],lang='fr')\n",
    "        clean_data_instance['d_e'] = [clean_word_list(split_sentence(i,lang='en'), lang='en') for i in babel_data[k]['d_e']]\n",
    "        clean_data_instance['d_c'] = [clean_word_list(split_sentence(i,lang='zh'), lang='zh') for i in babel_data[k]['d_c']]\n",
    "        clean_data_instance['d_f'] = [split_sentence(i,lang='fr') for i in babel_data[k]['d_f']]\n",
    "        for i in ['i_m','i','s']:\n",
    "            clean_data_instance[i] = babel_data[k][i]\n",
    "        clean_data[k] = clean_data_instance\n",
    "    return clean_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "clean_data = get_clean_data(babel_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "list(clean_data.values())[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pickle.dump(clean_data, open('../data/clean_data','wb'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "babel_data = pickle.load(open('../data/babel_data','rb'))\n",
    "print(len(babel_data))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "15755\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "list(babel_data.values())[0]"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'clean_data' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2a9626d4d423>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'clean_data' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "triple_lang_data = []\n",
    "for k in clean_data.keys():\n",
    "    if len(clean_data[k]['w_e']) > 0 and len(clean_data[k]['w_c']) > 0 and len(clean_data[k]['w_f']) > 0:\n",
    "        if len(clean_data[k]['d_e_m']) > 0 or len(clean_data[k]['d_e']) > 0:\n",
    "            if len(clean_data[k]['d_c_m']) > 0 or len(clean_data[k]['d_c']) > 0:\n",
    "                if len(clean_data[k]['d_f_m']) > 0 or len(clean_data[k]['d_f']) > 0:\n",
    "                    triple_lang_data.append(k)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(triple_lang_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "triple_lang_dataset = {'train':triple_lang_data[:6809], 'valid':triple_lang_data[6809:7660], 'test':triple_lang_data[7660:]}\n",
    "print(len(triple_lang_dataset['train']))\n",
    "print(len(triple_lang_dataset['valid']))\n",
    "print(len(triple_lang_dataset['test']))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pickle.dump(triple_lang_dataset,open('../data_set/triple_lang_data','wb'))"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}